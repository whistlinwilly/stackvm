Tuesday morning: that was easy!

When last we left, our protagonist had encountered an unexpected bug revealed
by a mundane cleanup (really another, but benign, bug fix).

Not being the kind to rush into battle, our hero first improved his vantage point:

- So I've got this "tracelog" tool that parses the output of `go test -v
  -stackvm.test.trace XXX` into a structural graph internally...
- it then uses this graph tot print out "full session logs" For each machine execution
- so in other words, you get all of the logs from all parents, and then the
  actual machine execution (it's a good deal more verbose, as it literally
  lives to strategically duplicate most of its input)
- anyhow, this tool needed a bit of work:
  - it never was handling unrecognized lines correctly
  - it never did recognize begin lines (masked by prior defect)
  - better support for in-situ "note" lines (i.e. the memory dump ones)

Here's a [recap of all that then][moar_tracelog].

Now, equipped with better trace logs, I really wanted to know: which memory
pages are getting shared and/or re-used between machines? As a recap, the
stackvm memory model works like:
- a page is a pointer to a `struct{r int32, d [64]byte}`
  - `r` is a ref count, updated atomically
  - `d` is the data
- a machine then has `pages []*page` for its memory:
  - pages get allocated lazily on write
  - reads return 0 for all locations in non-existant (`nil`) pages
  - if a write happens to a page with `r > 1`, then we copy the page and write
    to the copy (decrementig r on the old one of course...)
  - when a machine is copied (e.g. by a fork op), we just need to make a
    shallow copy of the `[]*page`, and increment all of their `r`s
  - once a machine has halted, and any value has been extracted from it, we
    return all of its pages to a pool for re-use

Because of all of this, my suspicion is that we had a bug somehere in the COW
mechanism. To debug that I really wanted to see the memory location backing
each part of `page.d` within each hex dump.

And [here's a recap of the new dumping][moar_dump].

Looking at my newly annotated trace log revealed that the problem was much more
banal: we just weren't zeroing re-used pages (thru the pool mechanism)!
- because the code was setup to use a `sync.Pool` directly (one put, many gets)
  the [easiest first move was to zero them eagerly when put back into the pool][814154d]
- but of course, [the real winning move is to only zero them on the way back out][d299ed6]
- but that first required [wrapping the `sync.Pool`][2a04394] so that we can do
  more in the get path

All that done, the truncation bug is dead! Tomorrow I can get back to finishing
out the `asm_data` feature!

- Josh

[moar_tracelog]: https://github.com/jcorbin/stackvm/compare/b9df8d325f5ffa10996bb766182af323b55425df...5301552e75e92394c4aacc344352543f2e7e5c01
[moar_dump]: https://github.com/jcorbin/stackvm/compare/5301552e75e92394c4aacc344352543f2e7e5c01...2607c17a9333c4a0227ddbaef18b250ae8c86b12
[814154d]: https://github.com/jcorbin/stackvm/commit/814154daaf896d0b62d0624185f5326c6ce39fb5
[d299ed6]: https://github.com/jcorbin/stackvm/commit/d299ed625e6a172fae5978795e07b23891609b36
[2a04394]: https://github.com/jcorbin/stackvm/commit/2a04394492a72293522eaf76379d11b6a55ba8cf
